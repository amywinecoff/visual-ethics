{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resources \n",
    "#https://towardsdatascience.com/building-an-image-color-analyzer-using-python-12de6b0acf74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "import colorsys\n",
    "import os\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "import contractions\n",
    "import spacy\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "from nltk import bigrams, trigrams\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import Request, urlopen\n",
    "import requests\n",
    "import string\n",
    "import regex as re\n",
    "import collections\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "ua = UserAgent()\n",
    "userAgent = ua.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_hex(rgb_color):\n",
    "    hex_color = \"#\"\n",
    "    for i in rgb_color:\n",
    "        i = int(i)\n",
    "        hex_color += (\"{:02x}\".format(i))\n",
    "    return hex_color\n",
    "\n",
    "def prep_image(raw_img):\n",
    "    modified_img = cv2.resize(raw_img, (900, 600), interpolation = cv2.INTER_AREA)\n",
    "    modified_img = modified_img.reshape(modified_img.shape[0]*modified_img.shape[1], 3)\n",
    "    return modified_img\n",
    "\n",
    "def color_analysis(img, url, k=5):\n",
    "    \n",
    "    clf = KMeans(n_clusters = k)\n",
    "    color_labels = clf.fit_predict(img)\n",
    "    center_colors = clf.cluster_centers_    \n",
    "    counts = Counter(color_labels)\n",
    "    ordered_colors = [center_colors[i].round(3).tolist() for i in counts.keys()]\n",
    "    hex_colors = [rgb_to_hex(ordered_colors[i]) for i in counts.keys()]    \n",
    "    plt.figure(figsize = (12, 8))\n",
    "    plt.pie(counts.values(), labels = hex_colors, colors = hex_colors)\n",
    "    plt.title('color proportions from {u}'.format(u=url))\n",
    "    plt.savefig('./plots/pie-' + url.replace('.', '-') + '.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "    color_array = np.array(list(counts.values()))\n",
    "    color_proportion = (color_array/sum(color_array)).round(3).tolist()\n",
    " \n",
    "    \n",
    "    color_dict = {}\n",
    "    \n",
    "    color_dict['centroids_rgb'] = ordered_colors\n",
    "    color_dict['centroids_hex'] = hex_colors\n",
    "    color_dict['color_counts'] = color_array.tolist()\n",
    "    color_dict['color_proportion'] = color_proportion\n",
    "    \n",
    "    hsv = [list(colorsys.rgb_to_hsv(c[0], c[1], c[2])) for c in color_dict['centroids_rgb']]\n",
    "    color_dict['centroids_hsv']=hsv\n",
    "    \n",
    "    #print(hex_colors)\n",
    "    return color_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_paragraph_text(url, hdr):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        req = Request(url,headers=hdr)\n",
    "        page = urlopen(req)\n",
    "    \n",
    "    except:\n",
    "        \n",
    "        print(\"http failed, trying https:\")\n",
    "        \n",
    "        try:\n",
    "            url = url.replace('http:', 'https:')\n",
    "            req = Request(url,headers=hdr)\n",
    "            page = urlopen(req)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"substitution of https also failed. Error is {}\".format(e))\n",
    "            \n",
    "    else:\n",
    "   \n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "        p=soup.find_all('p')\n",
    "        text = ' '.join(t.get_text() for t in p)\n",
    "\n",
    "        return text\n",
    "\n",
    "class TextPreprocessor(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        TBD\n",
    "        \"\"\"\n",
    "\n",
    "    def preprocess_df(self, df):\n",
    "        return df.apply(self._preprocess_text)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        doc = nlp(text)\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "\n",
    "        return self._lemmatize(removed_stop_words)\n",
    "\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t.text not in string.punctuation]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if not t.is_stop]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        \n",
    "        lemma_text = ' '.join([t.lemma_ for t in doc])\n",
    "        \n",
    "        return re.sub('\\s+',' ',lemma_text)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['www.uber.com', 'www.lyft.com', 'www.doordash.com']\n",
    "#urls = ['www.lyft.com']\n",
    "#check out https://github.com/robinreni96/Font_Recognition-DeepFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "options.add_argument(f'user-agent={userAgent}')\n",
    "    \n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.maximize_window()\n",
    "url_errors = {}\n",
    "urls_color_dicts = []\n",
    "\n",
    "#if the json already exists from a prior code run, delete it\n",
    "if os.path.exists('site_screenshot_colors.json'):\n",
    "    os.remove('site_screenshot_colors.json')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        sleep(2)\n",
    "        driver.get('https://' + url)\n",
    "        sleep(2)\n",
    "\n",
    "\n",
    "        S = lambda X: driver.execute_script('return document.body.parentNode.scroll'+X)\n",
    "        driver.set_window_size(S('Width'),S('Height')) # May need manual adjustment\n",
    "\n",
    "        el = driver.find_element(By.TAG_NAME, value='body')\n",
    "        el.screenshot('./site-screenshots/'+ url.replace('.', '-') + '.png')\n",
    "\n",
    "        image = cv2.imread('./site-screenshots/'+ url.replace('.', '-') + '.png')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(image)\n",
    "\n",
    "        modified_image = prep_image(image)\n",
    "        color_dict = color_analysis(modified_image, url)\n",
    "        color_dict['url'] = url\n",
    "        color_dict['screenshot'] = url.replace('.', '-') + '.png'\n",
    "        \n",
    "        \n",
    "        \n",
    "        with open('site_screenshot_colors.json', 'a') as f:\n",
    "            f.write(json.dumps(color_dict))\n",
    "            f.write('\\n')\n",
    "\n",
    "                                      \n",
    "\n",
    "        urls_color_dicts.append(color_dict)\n",
    "        \n",
    "        #font family is probably not reliable\n",
    "        print(el.value_of_css_property(\"font-family\"))\n",
    "        print(el.value_of_css_property(\"background-color\"))\n",
    "        print(el.value_of_css_property(\"color\"))\n",
    "        print(el.value_of_css_property(\"font-family\"))\n",
    "        print(el.value_of_css_property(\"font-size\"))\n",
    "        \n",
    "        \n",
    "        \n",
    "                \n",
    "    \n",
    "    except Exception as e:\n",
    "        \n",
    "        print(\"url {url} error {e}\".format(url=url, e=e))\n",
    "        \n",
    "        url_errors[url] = e\n",
    "\n",
    "#driver.quit()\n",
    "f.close()\n",
    "\n",
    "print(\"end...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_color_dicts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "full_url = \"http://\" + url\n",
    "# hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "#        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#        'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "#        'Accept-Encoding': 'none',\n",
    "#        'Accept-Language': 'en-US,en;q=0.8',\n",
    "#        'Connection': 'keep-alive'}\n",
    "\n",
    "hdr = {'User-Agent':str(ua.chrome)}\n",
    "\n",
    "req = Request(full_url,headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page, 'html.parser')\n",
    "\n",
    "main_site_text = get_paragraph_text(full_url, hdr).lower()\n",
    "#print(main_site_text)\n",
    "\n",
    "processed_text = TextPreprocessor().preprocess_text(main_site_text)\n",
    "print(processed_text)\n",
    "\n",
    "word_counts = collections.Counter(processed_text.split())\n",
    "# bg = bigrams(ah_text.split())\n",
    "# bigram_counts = collections.Counter(bg)\n",
    "# tg = trigrams(ah_text.split())\n",
    "# trigram_counts = collections.Counter(tg)\n",
    "\n",
    "word_counts.most_common(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts.most_common(50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#element = driver.find_element_by_tag_name('title')\n",
    "element = driver.find_element(By.TAG_NAME, value='title')\n",
    "\"/html/body\"\n",
    "properties = driver.execute_script('return window.getComputedStyle(arguments[0], null);', element)\n",
    "for property in properties:\n",
    "    print(property, element.value_of_css_property(property))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#element = driver.find_element(By.XPATH, \"/html/body\")\n",
    "#element = driver.find_element(By.TAG_NAME, value='title')\n",
    "\n",
    "print(el.value_of_css_property(\"background-color\"))\n",
    "print(el.value_of_css_property(\"color\"))\n",
    "print(el.value_of_css_property(\"font-family\"))\n",
    "print(el.value_of_css_property(\"font-size\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_elements(By.CSS_SELECTOR,\"@media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = driver.find_element(By.XPATH, \"/html/body\")\n",
    "properties = driver.execute_script('return window.getComputedStyle(arguments[0], null);', element)\n",
    "for property in properties:\n",
    "    print(property, element.value_of_css_property(property))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#are the rgb values, cluster centroids, cluster centroid proportions, or hsv values predictive of the type of business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "#https://www.bcorporation.net/en-us/find-a-b-corp/search?query=food\n",
    "\n",
    "body = {'query':'rune'} # <-- use 'query' not `Search'\n",
    "\n",
    "con = requests.post('http://services.runescape.com/m=itemdb_rs/results#main-search', data=body)\n",
    "print (con.content) # <-- print .content not .text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = requests.get('http://www.pharmacy295.gr/el/products/autocomplete.json',data = payload ,headers = headers)\n",
    "r = requests.get('https://www.bcorporation.net/en-us/find-a-b-corp/search?query=food/autocomplete.json',data = payload ,headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1'}\n",
    "\n",
    "payload = {\n",
    "    'query':'test',\n",
    "\n",
    "}\n",
    "r = requests.get('http://www.pharmacy295.gr/products',data = payload ,headers = headers)\n",
    "\n",
    "soup = BeautifulSoup(r.text,'lxml')\n",
    "products = soup.findAll('span', {'class':'name'})\n",
    "print(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "#import pandas as pd\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(url):\n",
    "    \"\"\"Return the source code for the provided URL. \n",
    "\n",
    "    Args: \n",
    "        url (string): URL of the page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        response (object): HTTP response object from requests_html. \n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        session = HTMLSession()\n",
    "        response = session.get(url)\n",
    "        return response\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_google(query):\n",
    "\n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.google.co.uk/search?q=\" + query)\n",
    "\n",
    "    links = list(response.html.absolute_links)\n",
    "    google_domains = ('https://www.google.', \n",
    "                      'https://google.', \n",
    "                      'https://webcache.googleusercontent.', \n",
    "                      'http://webcache.googleusercontent.', \n",
    "                      'https://policies.google.',\n",
    "                      'https://support.google.',\n",
    "                      'https://maps.google.')\n",
    "\n",
    "    for url in links[:]:\n",
    "        if url.startswith(google_domains):\n",
    "            links.remove(url)\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_source(\"https://www.google.co.uk/search?q=\" + query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "if asyncio.get_event_loop().is_running(): # Only patch if needed (i.e. running in Notebook, Spyder, etc)\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] Starting Chromium download.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 86.8M/86.8M [00:18<00:00, 4.81Mb/s]\n",
      "[INFO] Beginning extraction\n",
      "[INFO] Chromium extracted to: /Users/amywinecoff/Library/Application Support/pyppeteer/local-chromium/588429\n"
     ]
    }
   ],
   "source": [
    "# session = HTMLSession()\n",
    "# r = session.get('https://www.bcorporation.net/en-us/find-a-b-corp/search?q=food')\n",
    "\n",
    "# #r = session.get('http://www.yourjspage.com')\n",
    "\n",
    "# r.html.render()\n",
    "\n",
    "asession = AsyncHTMLSession()\n",
    "r = await asession.get('https://www.bcorporation.net/en-us/find-a-b-corp/search?q=food')\n",
    "await r.html.arender()\n",
    "resp=r.html.raw_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object HTML.arender at 0x7fb32a6eca40>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.html.arender()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"food\"\n",
    "response = get_source(\"https://www.bcorporation.net/en-us/find-a-b-corp/search?q=\" + query)\n",
    "\n",
    "session = HTMLSession()\n",
    "\n",
    "response.html.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = list(response.html.absolute_links)\n",
    "def get_results(query):\n",
    "    \n",
    "    query = urllib.parse.quote_plus(query)\n",
    "    response = get_source(\"https://www.bcorporation.net/en-us/find-a-b-corp/search?q=\" + query)\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_results(\"food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlpage = 'https://www.bcorporation.net/sitemap/sitemap-0.xml' \n",
    "page = urllib.request.urlopen(urlpage)\n",
    "# parse the html using beautiful soup and store in variable 'soup'\n",
    "soup = BeautifulSoup(page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xmlDict = {}\n",
    "\n",
    "r = requests.get(\"https://www.bcorporation.net/sitemap/sitemap-0.xml\")\n",
    "xml = r.text\n",
    "\n",
    "soup = BeautifulSoup(xml)\n",
    "sitemapTags = soup.find_all(\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = etree.fromstring(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9372"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this part\n",
    "r = requests.get(\"https://www.bcorporation.net/sitemap/sitemap-0.xml\")\n",
    "root = etree.fromstring(r.content)\n",
    "\n",
    "urls=[]\n",
    "for sitemap in root:\n",
    "    children = sitemap.getchildren()\n",
    "    #xmlDict[children[0].text] = children[1].text\n",
    "    if 'find-a-b-corp/company/' in children[0].text:\n",
    "        urls.append(children[0].text)\n",
    "#print (xmlDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.bcorporation.net/sitemap/sitemap-0.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
